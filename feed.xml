<?xml version="1.0" encoding="UTF-8"?>

<rss version="2.0"
  xmlns:content="http://purl.org/rss/1.0/modules/content/"
  xmlns:dc="http://purl.org/dc/elements/1.1/"
  xmlns:media="http://search.yahoo.com/mrss/"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:georss="http://www.georss.org/georss">

  <channel>
    <title>
      <![CDATA[  laserkelvin.github.io  ]]>
    </title>
    <link> https://laserkelvin.github.io/ </link>
    <description>
      <![CDATA[  A healthy mixture of professional and personal website and blog for Kelvin Lee  ]]>
    </description>
    <atom:link
      href="https://laserkelvin.github.io/feed.xml"
      rel="self"
      type="application/rss+xml" />


<item>
  <title>
    <![CDATA[  Using LLMs for daily tasks  ]]>
  </title>
  <link> https://laserkelvin.github.io/blog/2024/12/obsidian-smart-workflow/index.html </link>
  <guid> https://laserkelvin.github.io/blog/2024/12/obsidian-smart-workflow/index.html </guid>
  <description>
    <![CDATA[  Notes on how to configure Obsidian and Neovim for LLM use  ]]>
  </description>  
  
  <content:encoded>
    <![CDATA[  <h1 id="fill_title">Using LLMs for daily tasks</h1>
<p>I&#39;ve recently had the time of day to start playing around with incorporating LLMs into my usual workflows in earnest. Even though projects like <code>ollama</code> really lowered the bar of hosting LLMs, it&#39;s taken me a while of playing around with a variety of frontends and workflows such as Open WebUI to figure out exactly how exactly I want to use a locally hosted model for myself.</p>
<p>The effective problem statement is something like this: I use <code>nvim</code> for coding, and actually until quite recently, even for my notetaking with Telekasten.
    <label for="sn-1" class="margin-toggle sidenote-number"></label>
    <input type="checkbox" id="mn-1" class="margin-toggle"/>
    <span class="sidenote" id="sn-1">Nothing was wrong
with telekasten as a plugin; I never really got the hang of retrieval (i.e. finding the
note I needed at the right time. Maybe it's more intuitive for others, but I never really
had the time to practice.)</span>
     I&#39;ve since now moved to Obsidian for my notetaking, and it&#39;s been a pretty good experience so far. Thus, if I want to use an LLM for productivity, I would have to figure out how to serve both <code>nvim</code> and Obsidian.</p>
<h2 id="setup">Setup</h2>
<p>One of the big hurdles that I had to get over was actually not messing around with <code>docker-compose</code>. Normally, this would be the way I would go because it&#39;s the self-hosting way, but it introduced complexity that was really not needed to have to deal with ports, uptime, availability, etc. Instead, I finally got over myself and just installed it with the incredibly well packaged command given by the developers:</p>
<pre><code class="language-console">curl -fsSL https://ollama.com/install.sh | sh</code></pre>
<p>...this worked instantly on Manjaro, even creating a user group and installing it as a <code>systemctl</code> service which meant that I don&#39;t have to deal with ports, <code>ollama</code> will always be running as a background service
    <label for="sn-2" class="margin-toggle sidenote-number"></label>
    <input type="checkbox" id="mn-2" class="margin-toggle"/>
    <span class="sidenote" id="sn-2">If you were ever worried about taking up GPU memory like I was,
`ollama` actually clears unused models from memory after five minutes. It'll also handle loading
and unloading weights when switching between models.</span>
    , and every &quot;frontend&quot; I use can just map to the same server and model set. Clearly this was the design intention, but I liked to keep my system/service relatively clean, and I tend to overthink things.</p>
<p>I was then able to just run <code>ollama pull &lt;model_name&gt;</code>.</p>
<h2 id="llm_use_for_coding">LLM use for coding</h2>
<p>I&#39;ve been using <code>qwen2.5-coder:7b-instruct-q5_K_S</code> as a coding assistant. I&#39;ve found it to be somewhat adequate, albeit a little verbose. It is actually significantly better than the Llama 3.1 instruct models I&#39;ve used previously for coding, especially in terms of the quality of docstrings produced by the models.</p>
<p>In <code>nvim</code>, <a href="https://github.com/olimorris/codecompanion.nvim/tree/main"><code>codecompanion.nvim</code></a> has been a straightforward install and configure to use the background <code>ollama</code> service, needing just a little bit of tinkering to use <code>ollama</code> instead of the default ChatGPT remote service. Essentially, in addition to adding it to <code>lazy.nvim</code> as a dependency, I added a <code>~/.config/nvim/lua/config/companion.lua</code> file that then configures the &quot;strategy&quot; and &quot;adapters&quot;:</p>
<pre><code class="language-lua">require&#40;&quot;codecompanion&quot;&#41;.setup&#40;
  &#123;
    strategies &#61; &#123;
      chat &#61; &#123;
        adapter &#61; &quot;ollama&quot;
      &#125;,
      inline &#61; &#123;
        adapter &#61; &quot;ollama&quot;
      &#125;
    &#125;,
    adapters &#61; &#123;
    ollama &#61; function&#40;&#41;
      return require&#40;&quot;codecompanion.adapters&quot;&#41;.extend&#40;&quot;ollama&quot;, &#123;
        name &#61; &quot;qwen2.5-coder&quot;,
        schema &#61; &#123;
          model &#61; &#123;
            default &#61; &quot;qwen2.5-coder:7b-instruct-q5_K_S&quot;
          &#125;,
          num_ctx &#61; &#123;
            default &#61; 64000
          &#125;
        &#125;,
      &#125;&#41;
    end,
    &#125;,
&#125;&#41;</code></pre>
<p>This uses the same <code>ollama</code> model for both inline and chat completion tasks. As mentioned already, I&#39;ve used this for docstring generation &#40;The <code>&lt;C-a&gt;</code> keymap opens a chat window that I can just paste a function&#39;s worth to generate at a time&#41;, and because the model is so lightweight, generation is more or less instantaneous. Sometimes I do need to remind it to generate <strong>NumPy</strong> style docstrings, as it generates something close but not exactly correct in the argument type descriptions.</p>
<h2 id="obsidian_smart_brain">Obsidian Smart brain</h2>
<p>For notetaking, doing retrieval augmented generation &#40;RAG&#41; has been <strong>significantly</strong> more straightforward for information retrieval in my &#40;limited&#41; experience. I use <code>nomic-embed-text</code>, again provided by <code>ollama</code>, to index notes, and use <code>llama3.2:3b</code> for chat generation. The small model is incredibly quick on my dGPU, and will generate responses much faster than I can read them. The relevance and usefulness of the responses can vary, but I&#39;ve been pretty impressed so far when I ask questions about specific things I know exist in my notes so far. This functionality is provided by <a href="https://github.com/your-papa/obsidian-Smart2Brain"><code>obsidian-Smart2Brain</code></a>, which lets you configure <code>ollama</code> as the LLM backend for embeddings and for generation.</p>
<h2 id="conclusions">Conclusions</h2>
<p>I&#39;ll follow up a bit more as I get the hang of the workflows, but I&#39;m pretty keen to add more slash commands and utilities to <code>nvim</code> for my coding productivity. As far as notetaking goes, the plugin I&#39;m using now doesn&#39;t seem to provide an avenue for <em>workflows</em> &#40;i.e. agents&#41; just yet, but I&#39;m thinking of a few areas beyond autotagging and RAG; how do I improve my <strong>learning</strong> with LLM assistants?</p>
 ]]>
  </content:encoded>
    
  <pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate>  
  
  
  <atom:author>
    <atom:name>Kelvin Lee</atom:name>
  </atom:author>
        
</item>

<item>
  <title>
    <![CDATA[  Handy language model prompts for productivity  ]]>
  </title>
  <link> https://laserkelvin.github.io/blog/2024/08/useful-prompts/index.html </link>
  <guid> https://laserkelvin.github.io/blog/2024/08/useful-prompts/index.html </guid>
  <description>
    <![CDATA[  A letter to future to serve as a reminder to recenter myself throughout 2024.  ]]>
  </description>  
  
  <content:encoded>
    <![CDATA[  <h1 id="fill_title">Handy language model prompts for productivity</h1>
<p>Lately, I&#39;ve been increasingly using language models to help automate some of the nitty gritty stuff, including setting up a workflow for <a href="https://laserkelvin.github.io/research-paper-summaries">automating paper summarization</a>, and <em>some</em> assistance with misc. coding and productivity tasks.</p>
<p>While the models that can be served on a client CPU/GPU have substantially improved in the last two years, I&#39;ve found that they&#39;re still not quite at the point where they are reliable enough for me to pass more critical tasks. I still write all of my code, and even with RAG the paper summaries can be either very impressive &#40;pulling out specific figures and references&#41;, or on the other hand, <a href="https://en.wikipedia.org/wiki/Not_even_wrong">not even wrong</a>. In the former case it might just be a matter of coding style not meeting what I&#39;d like, but I would like to still try and continue to use LLMs where I find it comfortable to do so, which concretely translates to:</p>
<ol>
<li><p>Sufficient automation/relieves my cognitive load. It doesn&#39;t make sense for me to have an assistant where I don&#39;t trust their work, and I end up spending more time turning something unusable into something useful.</p>
</li>
<li><p>Fits into my natural workflow: I code more or less exclusively in Neovim now, to the point that using VSCode is actually quite jarring because of how I&#39;ve managed to tweak Neovim how I want it.
    <label for="sn-1" class="margin-toggle sidenote-number"></label>
    <input type="checkbox" id="mn-1" class="margin-toggle"/>
    <span class="sidenote" id="sn-1">It's not just vim-bindings, it's the whole plugin ecosystem! More on that in a post sometime.</span>
    </p>
</li>
</ol>
<h2 id="docstring_generation">Docstring generation</h2>
<p>Something that seems to fit squarely in this island of stability is docstring writing. In Neovim, I have the <code>nvim-llama</code> plugin that will start an <code>Ollama</code> container when I it, and I can access a chat prompt with the instruction fine-tuned Llama 3.1 8B. My workflow consists of writing code, and using the following prompt:</p>
<blockquote>
<p>You are a helpful AI assistant that specializes in generating high-quality docstrings for Python code functions.  Your task is to create docstrings that are:</p>
<ul>
<li><p>Accurate: Cover functionality, parameters, return values, and exceptions.</p>
</li>
<li><p>Concise: Brief and to the point, focusing on essential information.</p>
</li>
<li><p>Clear: Use simple language and avoid ambiguity.</p>
</li>
<li><p>Style: You must use NumPy style docstrings. When type hints/annotations are available, you must respect and use them.</p>
</li>
<li><p>Examples: Give examples in the docstring that demonstrate how the function should be used.</p>
</li>
<li><p>Generate docstring in this format:</p>
</li>
</ul>
<p>&quot;&quot;&quot;&lt;generated docstring&gt;&quot;&quot;&quot;.</p>
<p>The code snippet is given below:</p>
</blockquote>
<p>The prompt was largely taken from <a href="https://arxiv.org/html/2405.10243v1">arXiv:2405.10243v1</a>, but I&#39;ve added my own flair because I use NumPy style docstrings, and added the note for it to use type hints and annotations.</p>
<h2 id="simple_tasks">Simple tasks</h2>
<p>This kind of code generation is when it is faster to ask an LLM to do something in a language I don&#39;t use every day: good examples are <code>regex</code> and some kinds of shell tasks &#40;weirdly, I never got too much into <code>bash</code> hacking&#41;.</p>
<blockquote>
<p>You are a senior software engineer with many decades of experience working in shell prompts. You have been asked to provide a short script in a specified language to perform one task. As part of your response, you must abide by the following rules:</p>
<ul>
<li><p>Unless otherwise specified, use only the programming languages specified. Exceptions to this rule include cases like front and backend solutions &#40;e.g. CSS/HTML/Javascript&#41;.</p>
</li>
<li><p>Your response should be to the point; do not use greetings, filler, or engage in friendly conversation.</p>
</li>
<li><p>Try and keep the complexity of your snippet relatively low: it should be short but easily grokked.</p>
</li>
<li><p>Provide your response in a code block format, with the language annotated.</p>
</li>
</ul>
</blockquote>
 ]]>
  </content:encoded>
    
  <pubDate>Sun, 25 Aug 2024 00:00:00 +0000</pubDate>  
  
  
  <atom:author>
    <atom:name>Kelvin Lee</atom:name>
  </atom:author>
        
</item>

<item>
  <title>
    <![CDATA[  Things to live by in 2024  ]]>
  </title>
  <link> https://laserkelvin.github.io/blog/2024/01/things-to-live-by-2024/index.html </link>
  <guid> https://laserkelvin.github.io/blog/2024/01/things-to-live-by-2024/index.html </guid>
  <description>
    <![CDATA[  A letter to future to serve as a reminder to recenter myself throughout 2024.  ]]>
  </description>  
  
  <content:encoded>
    <![CDATA[  <h1 id="fill_title">Things to live by in 2024</h1>
<p>This past Christmas and new year season has been the most restful for me in recent memory: coming off the backside of the pandemic, making a career change, and moving across the country has occupied me the last few years. In this time, I&#39;ve been trying to take better care of myself, seeing the doctor, etc. on things that have been bothering me but I&#39;ve been putting off.</p>
<p>During this time, I&#39;ve been reading a lot of essays that get shared on Hacker News that I&#39;ve been finding quite insightful and inspiring. <a href="https://www.lesswrong.com/posts/uGDtroD26aLvHSoK2/dear-self-we-need-to-talk-about-ambition-1">One in particular</a> was written in the form of a letter to their younger self with a lot of advice and introspection that I found extremely relatable, and partly inspired me to write this up; instead of a letter to my past self&mdash;I don&#39;t think I&#39;m quite there yet&mdash;I thought I would write this up in the spirit of new year&#39;s resolutions, and serve to remind myself of more mindful times as the year inevitably picks up.</p>
<p>Without further ado, <em>things to live by in 2024</em>.</p>
<h2 id="look_after_yourself">Look after yourself</h2>
<p>If it hurts, take the time to make it better for yourself. &#40;Un&#41;fortunately, you&#39;re at an age now where your health&mdash;for better or worse&mdash;takes compound interest and your current state will propagate years down the line. When you were younger, you could brush stress, anxiety, and pain off and bounce back, but things like bad ergonomics are going to ruin you down the line if you&#39;re not careful. Do your best to exercise every day, take your Vitamin D, and fix your posture. It&#39;s not all that bad.</p>
<h2 id="spend_more_time_on_the_foundational_not_the_sensational">Spend more time on the foundational, not the sensational</h2>
<p>FOMO has been the gift that keeps on giving when it comes to stress and anxiety; you work in a fast moving field and there&#39;s just absolutely no way anybody can remain up to date with everything new that comes out. This includes reading new papers, and <em>especially</em> getting force fed by recommendation engines &#40;you&#39;re probably doom scrolling your Google feed&#41;. Cool new research, libraries, and &quot;stuff&quot; pop up at an hourly rate, and balancing between what you do for your job, your own personal interests, and your sanity, it&#39;s probably better for you to focus on moving averages as opposed to individual data points.
    <label for="sn-1" class="margin-toggle sidenote-number"></label>
    <input type="checkbox" id="mn-1" class="margin-toggle"/>
    <span class="sidenote" id="sn-1">\href{https://news.ycombinator.com/item?id=38829907}{This discussion} was particularly inspiring.</span>
     In other words, the vast majority of what you read about in 2024 are likely going to be one-off things, so try and look for things that are going to be here to stay and spend your time and effort there. <em>You&#39;ll always have more ideas than time</em>.</p>
<h2 id="its_okay_if_things_go_slowly_dont_go_to_plan_or_even_spiral_out_of_control">It&#39;s okay if things go slowly, don&#39;t go to plan, or even spiral out of control</h2>
<p>We do things not because they are easy, but because we thought they were going to be easy.
    <label for="sn-2" class="margin-toggle sidenote-number"></label>
    <input type="checkbox" id="mn-2" class="margin-toggle"/>
    <span class="sidenote" id="sn-2">I have no idea who to attribute this to, but evidently it's a mutation of JFK's Moon speech. Whoever it was, they were pretty cynical.</span>
     Consequently, things are inevitably going to be harder and take longer than you originally thought; <em>that&#39;s perfectly fine</em> and just be patient with yourself. It&#39;s okay to reduce the scope of things, and it&#39;s okay to not know how everything will pan out from the get go: if rules were made to be broken, then plans are made to go wrong. If there&#39;s one thing academia taught you is that projects&mdash;<em>actually your whole damn year</em>&mdash;will rarely go the way you thought it will. </p>
<p><a href="https://peoplelogic.ai/blog/history-of-objectives-and-key-results">OKRs</a> are good and all, but like all other models, it works for some things &#40;e.g. business targets&#41; and not others &#40;e.g. long term, dynamic projects&#41;. You can easily spend all of your time trying to meticulously craft OKRs to ensure you have the right scope, the right degree of ambition, but KRs and objectives can come crashing down when circumstances inevitably change. <em>Life finds a way</em>, so be fluid and be flexible.</p>
 ]]>
  </content:encoded>
    
  <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>  
  
  
  <atom:author>
    <atom:name>Kelvin Lee</atom:name>
  </atom:author>
        
</item>
</channel></rss>