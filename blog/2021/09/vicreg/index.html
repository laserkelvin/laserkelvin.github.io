<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin=anonymous > <link rel=stylesheet  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=icon  href="/assets/favicon.ico"> <title>Disentangling embeddings with self-supervised VICReg</title> <div id=layout > <div id=menu > <ul> <li><a href="/">Home</a> <li><a href="/blog/">Blog</a> <li><a href="/projects/">Projects</a> <li><a href="/resume/">CV</a> <li><a href="/about/">About me</a> </ul> </div> <div id=main > <div class=franklin-content ><h1 id=fill_title ><a href="#fill_title" class=header-anchor >Disentangling embeddings with self-supervised VICReg</a></h1> <p>Learning useful, <em>generalized</em> representations are universally important, however isn&#39;t a straightforward task. Conventional autoencoders and their variational counterparts will learn to produce embeddings that are useful, low dimensional representations, but they aren&#39;t guaranteed to fulfill things that we intuitively want to be included in these embeddings:</p> <ol> <li><p>Implicit clustering of similar data&mdash;can we encourage models to learn an unbiased heuristic? <label for=sn-unbiased  class="margin-toggle sidenote-number"></label> <input type=checkbox  id=mn-unbiased  class=margin-toggle /> <span class=sidenote  id=sn-unbiased >What distance measure is the best for what you're doing? If we want to follow the spirit of unsupervised learning, we let the data speak for itself.</span> </p> <li><p>Orthogonality&mdash;the data being encoded into each dimension should ideally be as uncorrelated with others as possible. <label for=sn-pca  class="margin-toggle sidenote-number"></label> <input type=checkbox  id=mn-pca  class=margin-toggle /> <span class=sidenote  id=sn-pca >Think principal components, and interpretability.</span> </p> </ol> <h2 id=contrastive_learning ><a href="#contrastive_learning" class=header-anchor >Contrastive learning</a></h2> <p>Conventionally, one way we can obtain good embeddings that can at least partially fulfill these points is through <a href="https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html">contrastive learning</a>. The idea behind contrastive metric learning is simple: your model learns to ascribe a margin/difference between like and unlike data, which can be achieved with both labeled or unlabeled data. The problem with contrastively learning is typically the computational cost: for each training example, we need to traverse the dataset to find contrastive examples of varying difficulty.</p> <h2 id=vicreg ><a href="#vicreg" class=header-anchor >VICReg</a></h2> <p>This approach takes an implicit approach to contrastive metric learning, decomposing the mechanisms to getting data similarity and orthogonality into <em>variance</em>, <em>invariance</em>, and <em>covariance</em> metrics as a regularization measure.</p> <p>The unfiltered notes can be <a href="https://laserkelvin.github.io/ml-reviews/notes/vicreg">found here.</a></p> <div class=page-foot > <div class=copyright > &copy; Kelvin Lee. Last modified: August 05, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and <a href="https://julialang.org">Julia</a>. </div> </div> </div> </div> </div>